
import os
import sys
import pandas as pd
import numpy as np
import json
from pathlib import Path

def process_ember_dataset(data_path="data/ember_dataset"):
    """EMBER dataset'ini iÅŸle"""
    
    print("ğŸ”¥ EMBER Dataset Ä°ÅŸleniyor...")
    
    # Data directory kontrolÃ¼
    ember_path = Path(data_path)
    if not ember_path.exists():
        print(f"âŒ {data_path} bulunamadÄ±!")
        print("ğŸ“¥ EMBER dataset'ini ÅŸuraya Ã§Ä±kart:")
        print(f"   {ember_path.absolute()}")
        return False
    
    # JSON dosyalarÄ±nÄ± ara
    json_files = list(ember_path.glob("**/*.jsonl")) + list(ember_path.glob("**/*.json"))
    
    if not json_files:
        print("âŒ JSON dosyalarÄ± bulunamadÄ±!")
        return False
    
    print(f"ğŸ“ {len(json_files)} JSON dosyasÄ± bulundu")
    
    # Ä°lk dosyayÄ± incele
    sample_file = json_files[0]
    print(f"ğŸ” Ã–rnek dosya: {sample_file}")
    
    try:
        with open(sample_file, 'r', encoding='utf-8', errors='replace') as f:
            sample_data = json.loads(f.readline())
        
        print(f"ğŸ“Š Ã–rnek veri yapÄ±sÄ±:")
        print(f"   ğŸ”‘ Keys: {list(sample_data.keys())}")
        
        if 'label' in sample_data:
            print(f"   ğŸ·ï¸ Label: {sample_data['label']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Dosya okuma hatasÄ±: {e}")
        return False

def process_kaggle_dataset(data_path="data/kaggle_malware"):
    """Kaggle Malware Classification dataset'ini iÅŸle"""
    
    print("ğŸ† Kaggle Malware Dataset Ä°ÅŸleniyor...")
    
    kaggle_path = Path(data_path)
    if not kaggle_path.exists():
        print(f"âŒ {data_path} bulunamadÄ±!")
        print("ğŸ“¥ Kaggle dataset'ini ÅŸuraya Ã§Ä±kart:")
        print(f"   {kaggle_path.absolute()}")
        return False
    
    # CSV dosyalarÄ±nÄ± ara
    csv_files = list(kaggle_path.glob("**/*.csv"))
    asm_files = list(kaggle_path.glob("**/*.asm"))
    
    print(f"ğŸ“ {len(csv_files)} CSV, {len(asm_files)} ASM dosyasÄ± bulundu")
    
    if csv_files:
        # Ä°lk CSV'yi incele
        sample_csv = csv_files[0]
        print(f"ğŸ” Ã–rnek CSV: {sample_csv}")
        
        try:
            df_sample = pd.read_csv(sample_csv, nrows=5, encoding='utf-8')
            print(f"ğŸ“Š CSV yapÄ±sÄ±:")
            print(f"   ğŸ“ Shape: {df_sample.shape}")
            print(f"   ğŸ”‘ Columns: {list(df_sample.columns)}")
            print("\nğŸ“‹ Ä°lk 3 satÄ±r:")
            print(df_sample.head(3))
            
        except Exception as e:
            print(f"âŒ CSV okuma hatasÄ±: {e}")
    
    return len(csv_files) > 0 or len(asm_files) > 0

def create_sample_dataset_if_missing():
    """EÄŸer hiÃ§ dataset yoksa kÃ¼Ã§Ã¼k Ã¶rnek oluÅŸtur"""
    
    print("ğŸ¯ KÃ¼Ã§Ã¼k Ã–rnek Dataset OluÅŸturuluyor...")
    
    # Data directory oluÅŸtur
    data_dir = Path("data/file_security")
    data_dir.mkdir(parents=True, exist_ok=True)
    
    # GerÃ§ekÃ§i dosya adlarÄ± ve Ã¶zellikleri
    sample_size = 5000
    
    malicious_patterns = [
        # ÅÃ¼pheli uzantÄ±lar
        "document.pdf.exe", "invoice.docx.scr", "photo.jpg.bat",
        "update.exe.com", "setup.msi.pif", "readme.txt.cmd",
        
        # Ã‡ift uzantÄ±
        "important.doc.exe", "bill.pdf.scr", "vacation.zip.bat",
        
        # ÅÃ¼pheli isimler
        "svchost.exe", "winlogon.scr", "explorer.bat",
        "system32.exe", "notepad.com", "calc.pif",
        
        # TÃ¼rkÃ§e tuzaklar
        "fatura.pdf.exe", "Ã¶zgeÃ§miÅŸ.docx.scr", "fotoÄŸraf.jpg.bat",
        "gÃ¼nceleme.exe.com", "program.msi.pif"
    ]
    
    benign_patterns = [
        # Normal programlar
        "setup.exe", "install.msi", "program.exe",
        "update.exe", "launcher.exe", "client.exe",
        
        # Sistem dosyalarÄ±
        "kernel32.dll", "user32.dll", "ntdll.dll",
        "msvcrt.dll", "advapi32.dll", "shell32.dll",
        
        # Belgeler (gÃ¼venli)
        "document.pdf", "spreadsheet.xlsx", "presentation.pptx",
        "image.jpg", "video.mp4", "audio.mp3"
    ]
    
    filenames = []
    labels = []
    features = []
    
    # Malicious dosyalar (50%)
    for i in range(sample_size // 2):
        base_name = np.random.choice(malicious_patterns)
        filename = f"{base_name.split('.')[0]}_{i}.{'.'.join(base_name.split('.')[1:])}"
        
        filenames.append(filename)
        labels.append(1)  # Malicious
        
        # Features Ã§Ä±kar
        feature_vector = extract_filename_features(filename, is_malicious=True)
        features.append(feature_vector)
    
    # Benign dosyalar (50%)
    for i in range(sample_size // 2):
        base_name = np.random.choice(benign_patterns)
        filename = f"{base_name.split('.')[0]}_{i}.{'.'.join(base_name.split('.')[1:])}"
        
        filenames.append(filename)
        labels.append(0)  # Benign
        
        # Features Ã§Ä±kar
        feature_vector = extract_filename_features(filename, is_malicious=False)
        features.append(feature_vector)
    
    # Feature names listesini oluÅŸtur
    feature_names = [
        'length', 'dot_count', 'digit_ratio', 'uppercase_ratio',
        'suspicious_ext', 'double_ext', 'entropy', 'vowel_ratio',
        'has_numbers', 'has_spaces', 'has_special_chars', 'has_system_name'
    ]

    # PE-like binary features ekle
    for i in range(12):
        feature_names.append(f'pe_feature_{i}')

    # DataFrame oluÅŸtur
    df = pd.DataFrame(data=features, columns=pd.Index(feature_names))
    df['filename'] = filenames
    df['label'] = labels
    
    # Train/test split
    train_size = int(0.8 * sample_size)
    indices = np.random.permutation(sample_size)
    
    train_indices = indices[:train_size]
    test_indices = indices[train_size:]
    
    train_df = df.iloc[train_indices]
    test_df = df.iloc[test_indices]
    
    # Save
    train_path = data_dir / "file_security_train.csv"
    test_path = data_dir / "file_security_test.csv"
    
    train_df.to_csv(train_path, index=False, encoding='utf-8')
    test_df.to_csv(test_path, index=False, encoding='utf-8')
    
    print(f"âœ… Ã–rnek dataset oluÅŸturuldu:")
    print(f"   ğŸ“ Train: {train_path} ({len(train_df)} samples)")
    print(f"   ğŸ“ Test: {test_path} ({len(test_df)} samples)")
    
    # Ä°statistikler
    print(f"\nğŸ“Š Dataset Ä°statistikleri:")
    print(f"   ğŸŸ¢ Benign: {len(df[df['label'] == 0])}")
    print(f"   ğŸ”´ Malicious: {len(df[df['label'] == 1])}")
    print(f"   ğŸ“ Ortalama uzunluk: {df['length'].mean():.1f}")
    print(f"   ğŸ“ˆ Ã‡ift uzantÄ± oranÄ±: %{df['double_ext'].mean()*100:.1f}")
    
    # Ã–rnekler gÃ¶ster
    print(f"\nğŸ” Ã–rnek Malicious Dosyalar:")
    malicious_samples = train_df[train_df['label'] == 1]['filename'].head(5)
    for filename in malicious_samples:
        print(f"   ğŸ”´ {filename}")
    
    print(f"\nğŸ” Ã–rnek Benign Dosyalar:")
    benign_samples = train_df[train_df['label'] == 0]['filename'].head(5)
    for filename in benign_samples:
        print(f"   ğŸŸ¢ {filename}")
    
    return True

def extract_filename_features(filename, is_malicious=False):
    """Dosya adÄ±ndan gÃ¼venlik Ã¶zelliklerini Ã§Ä±kar"""
    
    # Temel Ã¶zellikler
    length = len(filename)
    dot_count = filename.count('.')
    digit_ratio = len([c for c in filename if c.isdigit()]) / len(filename)
    uppercase_ratio = len([c for c in filename if c.isupper()]) / len(filename)
    
    # ÅÃ¼pheli uzantÄ±lar
    suspicious_exts = ['.scr', '.pif', '.bat', '.cmd', '.com', '.vbs', '.js']
    suspicious_ext = 1 if any(filename.lower().endswith(ext) for ext in suspicious_exts) else 0
    
    # Ã‡ift uzantÄ±
    parts = filename.split('.')
    double_ext = 1 if len(parts) > 2 else 0
    
    # Entropi (karmaÅŸÄ±klÄ±k)
    import math
    prob = [filename.count(c)/len(filename) for c in set(filename)]
    entropy = -sum(p * math.log2(p) for p in prob if p > 0)
    
    # Sesli harf oranÄ±
    vowels = 'aeiouAEIOU'
    vowel_ratio = len([c for c in filename if c in vowels]) / len(filename)
    
    # DiÄŸer Ã¶zellikler
    has_numbers = 1 if any(c.isdigit() for c in filename) else 0
    has_spaces = 1 if ' ' in filename else 0
    has_special_chars = 1 if any(c in '!@#$%^&*()+=[]{}|;:,<>?' for c in filename) else 0
    
    # Sistem dosya adlarÄ±
    system_names = ['svchost', 'winlogon', 'explorer', 'system32', 'notepad', 'calc']
    has_system_name = 1 if any(name in filename.lower() for name in system_names) else 0
    
    # Binary Ã¶zellikler (PE dosya simÃ¼lasyonu)
    pe_features = []
    if is_malicious:
        # Malicious dosyalar iÃ§in ÅŸÃ¼pheli deÄŸerler
        pe_features = [
            np.random.uniform(0.7, 1.0),  # High entropy sections
            np.random.uniform(0.6, 0.9),  # Suspicious imports ratio
            np.random.uniform(0.0, 0.3),  # Low legitimate API calls
            np.random.uniform(0.7, 1.0),  # High packed sections
            np.random.uniform(0.8, 1.0),  # Suspicious strings ratio
            np.random.uniform(0.0, 0.2),  # Low version info completeness
            np.random.uniform(0.9, 1.0),  # High obfuscation indicators
            np.random.uniform(0.7, 1.0),  # Suspicious section names
            np.random.uniform(0.0, 0.3),  # Low digital signature validity
            np.random.uniform(0.8, 1.0),  # High runtime packer indicators
            np.random.uniform(0.6, 0.9),  # Network activity indicators
            np.random.uniform(0.7, 1.0),  # Anti-analysis features
        ]
    else:
        # Benign dosyalar iÃ§in normal deÄŸerler
        pe_features = [
            np.random.uniform(0.2, 0.5),  # Normal entropy
            np.random.uniform(0.1, 0.4),  # Normal imports
            np.random.uniform(0.7, 1.0),  # High legitimate API usage
            np.random.uniform(0.0, 0.3),  # Low packed sections
            np.random.uniform(0.1, 0.4),  # Low suspicious strings
            np.random.uniform(0.7, 1.0),  # Complete version info
            np.random.uniform(0.0, 0.2),  # Low obfuscation
            np.random.uniform(0.1, 0.4),  # Normal section names
            np.random.uniform(0.7, 1.0),  # Valid digital signature
            np.random.uniform(0.0, 0.2),  # No runtime packers
            np.random.uniform(0.0, 0.3),  # Limited network activity
            np.random.uniform(0.0, 0.2),  # No anti-analysis
        ]
    
    # TÃ¼m Ã¶zellikleri birleÅŸtir
    features = [
        length, dot_count, digit_ratio, uppercase_ratio,
        suspicious_ext, double_ext, entropy, vowel_ratio,
        has_numbers, has_spaces, has_special_chars, has_system_name
    ] + pe_features
    
    return features

def main():
    """Ana iÅŸlev"""
    
    print("ğŸ¯ Manuel Dataset Ä°ÅŸleyici")
    print("=" * 50)
    
    # Hangi dataset'ler mevcut kontrol et
    datasets_found = []
    
    # EMBER kontrol
    if Path("data/ember_dataset").exists():
        datasets_found.append("EMBER")
        if process_ember_dataset():
            print("âœ… EMBER dataset hazÄ±r!")
    
    # Kaggle kontrol  
    if Path("data/kaggle_malware").exists():
        datasets_found.append("Kaggle")
        if process_kaggle_dataset():
            print("âœ… Kaggle dataset hazÄ±r!")
    
    # HiÃ§ dataset yoksa Ã¶rnek oluÅŸtur
    if not datasets_found:
        print("ğŸ“¥ HiÃ§ manuel dataset bulunamadÄ±.")
        print("ğŸ¯ KÃ¼Ã§Ã¼k Ã¶rnek dataset oluÅŸturuluyor...")
        
        if create_sample_dataset_if_missing():
            print("âœ… Ã–rnek dataset hazÄ±r!")
            datasets_found.append("Sample")
    
    if datasets_found:
        print(f"\nğŸš€ HazÄ±r dataset'ler: {', '.join(datasets_found)}")
        print("\nğŸ“ Sonraki adÄ±m:")
        print("   python scripts/train_file_model.py")
    else:
        print("\nâŒ HiÃ§ dataset hazÄ±rlanamadÄ±!")
        
        print("\nğŸ“¥ Manuel indirme seÃ§enekleri:")
        print("1. EMBER: https://github.com/elastic/ember/releases")
        print("2. Kaggle: https://www.kaggle.com/c/malware-classification") 
        print("3. BODMAS: https://whyisyoung.github.io/BODMAS/")

if __name__ == "__main__":
    main() 
